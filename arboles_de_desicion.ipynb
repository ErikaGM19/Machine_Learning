{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Análisis y Comparación de Modelos de Machine Learning: Árboles de Decisión**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto se busca construir modelos de **Machine Learning** que permitan predecir el estado de aprobación de un préstamo utilizando los atributos de las personas solicitantes. Para ello, se analiza un conjunto de datos que contiene 14 atributos, entre ellos la edad, el ingreso anual, el puntaje crediticio, el propósito del préstamo, entre otros. La etiqueta de clase, loan_status, indica si el préstamo fue aprobado (1) o rechazado (0).\n",
    "\n",
    "Utilizaremos la técnica de **Árboles de Decisión** para abordar este problema, variando sus configuraciones y modificando hiperparámetros clave. El objetivo es comparar la exactitud (accuracy) de los modelos generados y determinar cuáles ofrecen mejores predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### INTEGRANTES:\n",
    "  1. Marcela Mazo Castro - 1843612\n",
    "  2. Eyder Santiago Suárez Chávez - 2322714\n",
    "  3. Erika García Muñoz - 2259395\n",
    "  4. Juan José Moreno Jaramillo - 2310038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los Datos  \n",
    "En esta sección, se cargan y preparan los datos para el entrenamiento y evaluación de los modelos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Leer el archivo loan_data.csv\n",
    "data = pd.read_csv(\"loan_data.csv\")\n",
    "\n",
    "# Dividir aleatoriamente los datos\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=123)\n",
    "\n",
    "# Normalización y codificación de los datos\n",
    "numerical_features = [\"person_age\", \"person_income\", \"person_emp_exp\", \"loan_amnt\", \"loan_int_rate\", \n",
    "                      \"loan_percent_income\", \"cb_person_cred_hist_length\", \"credit_score\"]\n",
    "categorical_features = [\"person_gender\", \"person_education\", \"person_home_ownership\", \"loan_intent\", \n",
    "                        \"previous_loan_defaults_on_file\"]\n",
    "\n",
    "\n",
    "X_train = train_data.drop(columns=\"loan_status\")\n",
    "y_train = train_data[\"loan_status\"]\n",
    "X_test = test_data.drop(columns=\"loan_status\")\n",
    "y_test = test_data[\"loan_status\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(), categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Árboles de Decisión  \n",
    "Se construirán y evaluarán 10 árboles de decisión variando el hiperparámetro `max_depth` desde 1 hasta 10, utilizando dos criterios diferentes: `gini` y `entropy`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con criterion='gini':\n",
      "   Max Depth  Accuracy\n",
      "0          1  0.781778\n",
      "1          2  0.853556\n",
      "2          3  0.902556\n",
      "3          4  0.916444\n",
      "4          5  0.919111\n",
      "5          6  0.919222\n",
      "6          7  0.919556\n",
      "7          8  0.923778\n",
      "8          9  0.923222\n",
      "9         10  0.924444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 1 y 2. Leer datos y dividirlos\n",
    "data = pd.read_csv(\"loan_data.csv\")\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=123)\n",
    "\n",
    "# 3. Normalización y codificación\n",
    "# Reutilizar el preprocessor del notebook anterior\n",
    "\n",
    "# 4. Árboles de decisión con max_depth\n",
    "gini_results = []\n",
    "for depth in range(1, 11):\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", \n",
    "                                              max_depth=depth, random_state=123))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    gini_results.append({\"Max Depth\": depth, \"Accuracy\": accuracy})\n",
    "\n",
    "gini_results_df = pd.DataFrame(gini_results)\n",
    "\n",
    "# ## 5. Incluir en el notebook una tabla con el accuracy para los 10 árboles con criterion=gini\n",
    "print(\"Resultados con criterion='gini':\")\n",
    "print(gini_results_df)\n",
    "\n",
    "# Repetir el proceso con criterion=\"entropy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proceso usando el criterio=\"entropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados con criterion='entropy':\n",
      "   Max Depth  Accuracy\n",
      "0          1  0.781778\n",
      "1          2  0.853556\n",
      "2          3  0.902556\n",
      "3          4  0.915333\n",
      "4          5  0.916556\n",
      "5          6  0.920444\n",
      "6          7  0.920667\n",
      "7          8  0.923778\n",
      "8          9  0.925222\n",
      "9         10  0.924556\n"
     ]
    }
   ],
   "source": [
    "# ## 6. Repetir el mismo procedimiento con criterion=entropy\n",
    "entropy_results = []\n",
    "for depth in range(1, 11):\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", \n",
    "                                              max_depth=depth, random_state=123))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    entropy_results.append({\"Max Depth\": depth, \"Accuracy\": accuracy})\n",
    "\n",
    "entropy_results_df = pd.DataFrame(entropy_results)\n",
    "\n",
    "# ## 7. Incluir en el notebook una tabla con el accuracy para los 10 árboles con criterion=entropy\n",
    "print(\"\\nResultados con criterion='entropy':\")\n",
    "print(entropy_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con criterion='gini':\n",
      "   Max Depth  Accuracy\n",
      "0          1  0.781778\n",
      "1          2  0.853556\n",
      "2          3  0.902556\n",
      "3          4  0.916444\n",
      "4          5  0.919111\n",
      "5          6  0.919222\n",
      "6          7  0.919556\n",
      "7          8  0.923778\n",
      "8          9  0.923222\n",
      "9         10  0.924444\n",
      "\n",
      "Resultados con criterion='entropy':\n",
      "   Max Depth  Accuracy\n",
      "0          1  0.781778\n",
      "1          2  0.853556\n",
      "2          3  0.902556\n",
      "3          4  0.915333\n",
      "4          5  0.916556\n",
      "5          6  0.920444\n",
      "6          7  0.920667\n",
      "7          8  0.923778\n",
      "8          9  0.925222\n",
      "9         10  0.924556\n"
     ]
    }
   ],
   "source": [
    "# ## 8. Indicar los hiperparámetros que permiten obtener el árbol con mayor accuracy\n",
    "# Vamos a encontrar el máximo accuracy entre ambos criterios\n",
    "\n",
    "#Estos son los resultados   \n",
    "print(\"Resultados con criterion='gini':\")\n",
    "print(gini_results_df)\n",
    "\n",
    "print(\"\\nResultados con criterion='entropy':\")\n",
    "print(entropy_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estilos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colors:\n",
    "    HEADER = '\\033[95m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "\n",
    "def print_pretty_table(df, enumerate_rows=False):\n",
    "    if enumerate_rows:\n",
    "        df = df.copy()\n",
    "        df.insert(0, 'Red', range(0, len(df)))\n",
    "\n",
    "    df_formatted = df.copy()\n",
    "    for col in df_formatted.columns:\n",
    "        if col != 'Alpha' and (df_formatted[col].dtype == 'float' or df_formatted[col].dtype == 'int'):\n",
    "            def format_num(x):\n",
    "                if isinstance(x, float):\n",
    "                    return f\"{x:.6f}\"\n",
    "                elif isinstance(x, int):\n",
    "                    return str(x)\n",
    "                else:\n",
    "                    return str(x)\n",
    "            df_formatted[col] = df_formatted[col].apply(format_num)\n",
    "\n",
    "    columns = list(df_formatted.columns)\n",
    "    col_widths = [\n",
    "        max(len(str(item)) for item in [col] + df_formatted[col].astype(str).tolist()) + 2\n",
    "        for col in columns\n",
    "    ]\n",
    "    top_left = '┌'\n",
    "    top_right = '┐'\n",
    "    bottom_left = '└'\n",
    "    bottom_right = '┘'\n",
    "    horizontal = '─'\n",
    "    vertical = '│'\n",
    "    top_sep = '┬'\n",
    "    bottom_sep = '┴'\n",
    "    mid_sep = '┼'\n",
    "    left_sep = '├'\n",
    "    right_sep = '┤'\n",
    "    middle_sep = '┼'\n",
    "\n",
    "    def create_border(left, sep, right):\n",
    "        return left + sep.join([horizontal * width for width in col_widths]) + right\n",
    "\n",
    "    def create_row(items, colored=False):\n",
    "        if colored:\n",
    "            return vertical + vertical.join([\n",
    "                f\"{Colors.BOLD + Colors.YELLOW}{str(item).center(width)}{Colors.ENDC}\"\n",
    "                for item, width in zip(items, col_widths)\n",
    "            ]) + vertical\n",
    "        else:\n",
    "            return vertical + vertical.join([str(item).center(width) for item, width in zip(items, col_widths)]) + vertical\n",
    "\n",
    "    print(create_border(top_left, horizontal, top_right))\n",
    "    print(create_row(columns, colored=True))\n",
    "    print(create_border(left_sep, mid_sep, right_sep))\n",
    "    for i, row_data in df_formatted.iterrows():\n",
    "        if i % 2 == 0:\n",
    "            print(create_row(row_data, colored=False))\n",
    "        else:\n",
    "            row_str = vertical\n",
    "            for item, width in zip(row_data, col_widths):\n",
    "                row_str += f\"\\033[48;5;240m{str(item).center(width)}\\033[0m{vertical}\"\n",
    "            print(row_str)\n",
    "    print(create_border(bottom_left, horizontal, bottom_right))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ver Tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejores resultados:\n",
      "Mejor resultado con Gini:\n",
      "┌──────────────────────┐\n",
      "│\u001b[1m\u001b[93m Max Depth \u001b[0m│\u001b[1m\u001b[93m Accuracy \u001b[0m│\n",
      "├───────────┼──────────┤\n",
      "│\u001b[48;5;240m     10    \u001b[0m│\u001b[48;5;240m 0.924444 \u001b[0m│\n",
      "└──────────────────────┘\n",
      "Mejor resultado con Entropy:\n",
      "┌──────────────────────┐\n",
      "│\u001b[1m\u001b[93m Max Depth \u001b[0m│\u001b[1m\u001b[93m Accuracy \u001b[0m│\n",
      "├───────────┼──────────┤\n",
      "│     9     │ 0.925222 │\n",
      "└──────────────────────┘\n",
      "\n",
      "El mejor árbol en general se obtuvo con criterion='entropy', con hiperparámetros:\n",
      "┌──────────────────────┐\n",
      "│\u001b[1m\u001b[93m Max Depth \u001b[0m│\u001b[1m\u001b[93m Accuracy \u001b[0m│\n",
      "├───────────┼──────────┤\n",
      "│     9     │ 0.925222 │\n",
      "└──────────────────────┘\n",
      "\n",
      "Hiperparámetros iniciales con mayor accuracy: criterion=entropy, splitter='best', max_depth=9, random_state=123\n",
      "min_samples_split=2, Accuracy=0.9252222222222222\n",
      "min_samples_split=10, Accuracy=0.9251111111111111\n",
      "\n",
      "Resultados variando min_samples_split:\n",
      "┌──────────────────────────────┐\n",
      "│\u001b[1m\u001b[93m min_samples_split \u001b[0m│\u001b[1m\u001b[93m Accuracy \u001b[0m│\n",
      "├───────────────────┼──────────┤\n",
      "│         2         │ 0.925222 │\n",
      "│\u001b[48;5;240m         10        \u001b[0m│\u001b[48;5;240m 0.925111 \u001b[0m│\n",
      "└──────────────────────────────┘\n",
      "La exactitud se mantiene igual con min_samples_split=2.0.\n",
      "La exactitud empeora con min_samples_split=10.0.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMejores resultados:\")\n",
    "print(\"Mejor resultado con Gini:\")\n",
    "print_pretty_table(best_gini)\n",
    "\n",
    "print(\"Mejor resultado con Entropy:\")\n",
    "print_pretty_table(best_entropy)\n",
    "\n",
    "# Determinación del mejor modelo en general\n",
    "if max_gini_acc > max_entropy_acc:\n",
    "    print(\"\\nEl mejor árbol en general se obtuvo con criterion='gini', con hiperparámetros:\")\n",
    "    print_pretty_table(best_gini)\n",
    "else:\n",
    "    print(\"\\nEl mejor árbol en general se obtuvo con criterion='entropy', con hiperparámetros:\")\n",
    "    print_pretty_table(best_entropy)\n",
    "\n",
    "# Seleccionamos el mejor modelo\n",
    "if max_entropy_acc >= max_gini_acc:\n",
    "    best_criterion = \"entropy\"\n",
    "    best_depth = best_entropy[\"Max Depth\"].values[0]\n",
    "    best_acc = max_entropy_acc\n",
    "else:\n",
    "    best_criterion = \"gini\"\n",
    "    best_depth = best_gini[\"Max Depth\"].values[0]\n",
    "    best_acc = max_gini_acc\n",
    "\n",
    "print(f\"\\nHiperparámetros iniciales con mayor accuracy: criterion={best_criterion}, splitter='best', max_depth={best_depth}, random_state=123\")\n",
    "\n",
    "variations = [2, 10]  # min_samples_split variaciones\n",
    "variation_results = []\n",
    "for mss in variations:\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', DecisionTreeClassifier(criterion=best_criterion, splitter=\"best\", \n",
    "                                              max_depth=int(best_depth), random_state=123,\n",
    "                                              min_samples_split=mss))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    variation_results.append({\"min_samples_split\": mss, \"Accuracy\": accuracy})\n",
    "    print(f\"min_samples_split={mss}, Accuracy={accuracy}\")\n",
    "\n",
    "var_df = pd.DataFrame(variation_results)\n",
    "print(\"\\nResultados variando min_samples_split:\")\n",
    "print_pretty_table(var_df)\n",
    "\n",
    "# Analizamos la mejora, empeora o mantiene\n",
    "for i, row in var_df.iterrows():\n",
    "    if float(row[\"Accuracy\"]) > best_acc:\n",
    "        print(f\"La exactitud mejora con min_samples_split={row['min_samples_split']}.\")\n",
    "    elif float(row[\"Accuracy\"]) < best_acc:\n",
    "        print(f\"La exactitud empeora con min_samples_split={row['min_samples_split']}.\")\n",
    "    else:\n",
    "        print(f\"La exactitud se mantiene igual con min_samples_split={row['min_samples_split']}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusiones**\n",
    "\n",
    "De acuerdo con los resultados obtenidos al variar el hiperparámetro `max_depth` entre **1** y **10**:\n",
    "\n",
    "- **Con `criterion='gini'`:**  \n",
    "  El mejor resultado se obtuvo con `max_depth=10`, con una exactitud de aproximadamente **0.9244**.\n",
    "\n",
    "- **Con `criterion='entropy'`:**  \n",
    "  El mejor resultado se obtuvo con `max_depth=9`, logrando una exactitud de aproximadamente **0.9252**.\n",
    "\n",
    "Al comparar ambos árboles, el **árbol con `criterion='entropy'`, `splitter='best'`, `max_depth=9`, `random_state=123`** obtuvo el **mayor accuracy (0.9252)**, siendo esta la mejor combinación de hiperparámetros en las pruebas realizadas hasta el momento.\n",
    "\n",
    "\n",
    "### **Análisis al variar el hiperparámetro `min_samples_split`**\n",
    "\n",
    "Tomando el mejor árbol encontrado (**criterion=entropy, splitter=best, max_depth=9, random_state=123**), se probó el hiperparámetro `min_samples_split` con dos valores diferentes:\n",
    "\n",
    "- **min_samples_split=2:** Accuracy ≈ **0.9252**\n",
    "- **min_samples_split=10:** Accuracy ≈ **0.9251**\n",
    "\n",
    "Al incrementar este hiperparámetro de **2** a **10**, la exactitud se redujo ligeramente, de **0.9252** a **0.9251**. Esto noa indica que, con estos datos y configuraciones, a un mayor valor de `min_samples_split` **no aportó una mejora** en el rendimiento; por el contrario, **disminuyó ligeramente** la exactitud. Por lo tanto, mantener `min_samples_split` en su valor por defecto (**2**) nos resulta más adecuado.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
