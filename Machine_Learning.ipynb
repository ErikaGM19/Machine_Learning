{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Análisis y Comparación de Modelos de Machine Learning: Redes Neuronales y Árboles de Decisión**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto se busca construir modelos de **Machine Learning** que permitan predecir el estado de aprobación de un préstamo utilizando los atributos de las personas solicitantes. Para ello, se analiza un conjunto de datos que contiene 14 atributos, entre ellos la edad, el ingreso anual, el puntaje crediticio, el propósito del préstamo, entre otros. La etiqueta de clase, loan_status, indica si el préstamo fue aprobado (1) o rechazado (0).\n",
    "\n",
    "Utilizaremos dos técnicas principales para abordar este problema: **Redes Neuronales** y **Árboles de Decisión**, variando sus configuraciones y modificando hiperparámetros clave. El objetivo es comparar la exactitud (accuracy) de los modelos generados y determinar cuáles ofrecen mejores predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### INTEGRANTES:\n",
    "  1. Marcela Mazo Castro - 1843612\n",
    "  2. Eyder Santiago Suárez Chávez - 2322714\n",
    "  3. Erika García Muñoz - 2259395\n",
    "  4. Juan José Moreno Jaramillo - 2310038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los Datos  \n",
    "En esta sección, se cargan y preparan los datos para el entrenamiento y evaluación de los modelos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, OneHotEncoder\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Leer el archivo loan_data.csv\n",
    "data = pd.read_csv(\"loan_data.csv\")\n",
    "\n",
    "# Dividir aleatoriamente los datos\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=123)\n",
    "\n",
    "# Normalización y codificación de los datos\n",
    "numerical_features = [\"person_age\", \"person_income\", \"person_emp_exp\", \"loan_amnt\", \"loan_int_rate\", \n",
    "                      \"loan_percent_income\", \"cb_person_cred_hist_length\", \"credit_score\"]\n",
    "categorical_features = [\"person_gender\", \"person_education\", \"person_home_ownership\", \"loan_intent\", \n",
    "                        \"previous_loan_defaults_on_file\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(), categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Redes Neuronales  \n",
    "En esta sección, se construyen y evalúan 5 modelos de redes neuronales variando la cantidad de capas ocultas, el número de neuronas por capa y los hiperparámetros principales como solver y activation. La implementación utiliza la clase MLPClassifier de scikit-learn para entrenar las redes neuronales con diferentes configuraciones topológicas y evaluar su desempeño en términos de exactitud.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Construcción de redes neuronales\n",
    "topologies = [\n",
    "    (50,), (100,), (50, 50), (100, 50), (100, 100)\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for topology in topologies:\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', MLPClassifier(hidden_layer_sizes=topology, random_state=123, max_iter=500))\n",
    "    ])\n",
    "    \n",
    "    # Entrenamiento\n",
    "    model.fit(train_data.drop(columns=\"loan_status\"), train_data[\"loan_status\"])\n",
    "    \n",
    "    # Predicción\n",
    "    predictions = model.predict(test_data.drop(columns=\"loan_status\"))\n",
    "    accuracy = accuracy_score(test_data[\"loan_status\"], predictions)\n",
    "    \n",
    "    results.append({\"Topology\": topology, \"Accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de Resultados  \n",
    "Los resultados de las diferentes configuraciones topológicas se resumen en una tabla, mostrando la exactitud alcanzada por cada red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tabla de resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# 3. Hiperparámetro adicional\n",
    "# Modificar un hiperparámetro adicional (como alpha)\n",
    "alpha_variations = [0.0001, 0.01]\n",
    "\n",
    "for alpha in alpha_variations:\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', MLPClassifier(hidden_layer_sizes=(100, 50), random_state=123, max_iter=500, alpha=alpha))\n",
    "    ])\n",
    "    \n",
    "    model.fit(train_data.drop(columns=\"loan_status\"), train_data[\"loan_status\"])\n",
    "    predictions = model.predict(test_data.drop(columns=\"loan_status\"))\n",
    "    accuracy = accuracy_score(test_data[\"loan_status\"], predictions)\n",
    "    \n",
    "    print(f\"Alpha: {alpha}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Árboles de Decisión  \n",
    "Se construirán y evaluarán 10 árboles de decisión variando el hiperparámetro `max_depth` desde 1 hasta 10, utilizando dos criterios diferentes: `gini` y `entropy`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Max Depth  Accuracy\n",
      "0          1  0.781778\n",
      "1          2  0.853556\n",
      "2          3  0.902556\n",
      "3          4  0.916444\n",
      "4          5  0.919111\n",
      "5          6  0.919222\n",
      "6          7  0.919556\n",
      "7          8  0.923778\n",
      "8          9  0.923222\n",
      "9         10  0.924444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 1 y 2. Leer datos y dividirlos\n",
    "data = pd.read_csv(\"loan_data.csv\")\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=123)\n",
    "\n",
    "# 3. Normalización y codificación\n",
    "# Reutilizar el preprocessor del notebook anterior\n",
    "\n",
    "# 4. Árboles de decisión con max_depth\n",
    "depth_results = []\n",
    "\n",
    "for depth in range(1, 11):\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", max_depth=depth, random_state=123))\n",
    "    ])\n",
    "    \n",
    "    model.fit(train_data.drop(columns=\"loan_status\"), train_data[\"loan_status\"])\n",
    "    predictions = model.predict(test_data.drop(columns=\"loan_status\"))\n",
    "    accuracy = accuracy_score(test_data[\"loan_status\"], predictions)\n",
    "    \n",
    "    depth_results.append({\"Max Depth\": depth, \"Accuracy\": accuracy})\n",
    "\n",
    "depth_results_df = pd.DataFrame(depth_results)\n",
    "print(depth_results_df)\n",
    "\n",
    "# Repetir el proceso con criterion=\"entropy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones de los Modelos "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
